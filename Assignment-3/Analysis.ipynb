{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "congressional-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polyphonic-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spark-sql').master('local').getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "filepath = 'data/Seasons_Stats.csv'\n",
    "df = sqlContext.read.load(filepath, format='com.databricks.spark.csv', header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fixed-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('seasons_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "banned-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     _c0|      int|   null|\n",
      "|    Year|      int|   null|\n",
      "|  Player|   string|   null|\n",
      "|     Pos|   string|   null|\n",
      "|     Age|      int|   null|\n",
      "|      Tm|   string|   null|\n",
      "|       G|      int|   null|\n",
      "|      GS|      int|   null|\n",
      "|      MP|      int|   null|\n",
      "|     PER|   double|   null|\n",
      "|     TS%|   double|   null|\n",
      "|    3PAr|   double|   null|\n",
      "|     FTr|   double|   null|\n",
      "|    ORB%|   double|   null|\n",
      "|    DRB%|   double|   null|\n",
      "|    TRB%|   double|   null|\n",
      "|    AST%|   double|   null|\n",
      "|    STL%|   double|   null|\n",
      "|    BLK%|   double|   null|\n",
      "|    TOV%|   double|   null|\n",
      "|    USG%|   double|   null|\n",
      "|   blanl|   string|   null|\n",
      "|     OWS|   double|   null|\n",
      "|     DWS|   double|   null|\n",
      "|      WS|   double|   null|\n",
      "|   WS/48|   double|   null|\n",
      "|  blank2|   string|   null|\n",
      "|    OBPM|   double|   null|\n",
      "|    DBPM|   double|   null|\n",
      "|     BPM|   double|   null|\n",
      "|    VORP|   double|   null|\n",
      "|      FG|      int|   null|\n",
      "|     FGA|      int|   null|\n",
      "|     FG%|   double|   null|\n",
      "|      3P|      int|   null|\n",
      "|     3PA|      int|   null|\n",
      "|     3P%|   double|   null|\n",
      "|      2P|      int|   null|\n",
      "|     2PA|      int|   null|\n",
      "|     2P%|   double|   null|\n",
      "|    eFG%|   double|   null|\n",
      "|      FT|      int|   null|\n",
      "|     FTA|      int|   null|\n",
      "|     FT%|   double|   null|\n",
      "|     ORB|      int|   null|\n",
      "|     DRB|      int|   null|\n",
      "|     TRB|      int|   null|\n",
      "|     AST|      int|   null|\n",
      "|     STL|      int|   null|\n",
      "|     BLK|      int|   null|\n",
      "|     TOV|      int|   null|\n",
      "|      PF|      int|   null|\n",
      "|     PTS|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe = sqlContext.sql(\"describe seasons_stats\")\n",
    "describe.show(n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "alone-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " _c0    | 0               \n",
      " Year   | 1950            \n",
      " Player | Curly Armstrong \n",
      " Pos    | G-F             \n",
      " Age    | 31              \n",
      " Tm     | FTW             \n",
      " G      | 63              \n",
      " GS     | null            \n",
      " MP     | null            \n",
      " PER    | null            \n",
      " TS%    | 0.368           \n",
      " 3PAr   | null            \n",
      " FTr    | 0.467           \n",
      " ORB%   | null            \n",
      " DRB%   | null            \n",
      " TRB%   | null            \n",
      " AST%   | null            \n",
      " STL%   | null            \n",
      " BLK%   | null            \n",
      " TOV%   | null            \n",
      " USG%   | null            \n",
      " blanl  | null            \n",
      " OWS    | -0.1            \n",
      " DWS    | 3.6             \n",
      " WS     | 3.5             \n",
      " WS/48  | null            \n",
      " blank2 | null            \n",
      " OBPM   | null            \n",
      " DBPM   | null            \n",
      " BPM    | null            \n",
      " VORP   | null            \n",
      " FG     | 144             \n",
      " FGA    | 516             \n",
      " FG%    | 0.279           \n",
      " 3P     | null            \n",
      " 3PA    | null            \n",
      " 3P%    | null            \n",
      " 2P     | 144             \n",
      " 2PA    | 516             \n",
      " 2P%    | 0.279           \n",
      " eFG%   | 0.279           \n",
      " FT     | 170             \n",
      " FTA    | 241             \n",
      " FT%    | 0.705           \n",
      " ORB    | null            \n",
      " DRB    | null            \n",
      " TRB    | null            \n",
      " AST    | 176             \n",
      " STL    | null            \n",
      " BLK    | null            \n",
      " TOV    | null            \n",
      " PF     | 217             \n",
      " PTS    | 458             \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-psychology",
   "metadata": {},
   "source": [
    "### Best scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "straight-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|            Player|points|\n",
      "+------------------+------+\n",
      "|      Karl Malone*| 13528|\n",
      "|   Michael Jordan*| 12192|\n",
      "|     Eddie Johnson| 11896|\n",
      "|       Kobe Bryant| 11719|\n",
      "| Shaquille O'Neal*| 11661|\n",
      "|  Hakeem Olajuwon*| 10749|\n",
      "|     Dirk Nowitzki| 10688|\n",
      "|Dominique Wilkins*| 10661|\n",
      "|     Kevin Garnett| 10648|\n",
      "|      LeBron James| 10423|\n",
      "|        Tim Duncan| 10285|\n",
      "|     Alex English*| 10174|\n",
      "|      Vince Carter|  9961|\n",
      "|    Patrick Ewing*|  9702|\n",
      "|    Allen Iverson*|  9532|\n",
      "|      Gary Payton*|  9373|\n",
      "|   Carmelo Anthony|  9300|\n",
      "|         Ray Allen|  9165|\n",
      "|    Clyde Drexler*|  8906|\n",
      "|       Paul Pierce|  8668|\n",
      "+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectall= sqlContext.sql(\"SELECT Player, sum(2P + 3P) as points from seasons_stats group by Player order by points desc\")\n",
    "selectall.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "single-marshall",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 251, in init_stream_yield_batches\n    for series in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper\n    return f(keys, vals)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 160, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-7d4a757b2449>\", line 2, in sum_points\n  File \"/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py\", line 5462, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'unique'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3656d1b2be54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Player'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyInPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Player string, suma float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 258, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 251, in init_stream_yield_batches\n    for series in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper\n    return f(keys, vals)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 160, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-61-7d4a757b2449>\", line 2, in sum_points\n  File \"/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py\", line 5462, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'unique'\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Player').applyInPandas(sum_points, schema = 'Player string, suma float').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-wisdom",
   "metadata": {},
   "source": [
    "### 3 Points attempts per season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ahead-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|           average|\n",
      "+----+------------------+\n",
      "|1980|15.467787114845938|\n",
      "|1981|11.462809917355372|\n",
      "|1982|12.479892761394101|\n",
      "|1983|11.744245524296675|\n",
      "|1984|13.408163265306122|\n",
      "|1985|16.596685082872927|\n",
      "|1986| 17.61741424802111|\n",
      "|1987| 24.27777777777778|\n",
      "|1988|25.199530516431924|\n",
      "|1989|  33.0958904109589|\n",
      "|1990| 33.46623093681917|\n",
      "|1991| 37.61224489795919|\n",
      "|1992| 38.06331877729258|\n",
      "|1993| 45.39866369710467|\n",
      "|1994| 47.88149688149688|\n",
      "|1995| 79.36725663716814|\n",
      "|1996| 76.21100917431193|\n",
      "|1997| 76.67421602787456|\n",
      "|1998| 62.20475319926874|\n",
      "|1999| 39.96252465483235|\n",
      "|2000| 68.12903225806451|\n",
      "|2001| 63.85474860335196|\n",
      "|2002|            75.034|\n",
      "|2003|  76.8136645962733|\n",
      "|2004| 67.87692307692308|\n",
      "|2005| 75.05299145299145|\n",
      "|2006| 76.19538188277087|\n",
      "|2007| 85.79263565891473|\n",
      "|2008| 81.48571428571428|\n",
      "|2009|  86.8298969072165|\n",
      "|2010| 85.09342560553634|\n",
      "|2011|            84.664|\n",
      "|2012| 69.46823956442832|\n",
      "|2013| 92.42233856893543|\n",
      "|2014| 95.64320785597381|\n",
      "|2015| 97.41935483870968|\n",
      "|2016|110.53460207612457|\n",
      "|2017|122.91764705882353|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectall= sqlContext.sql(\"SELECT Year, AVG(3PA) as average from seasons_stats where 3PA is not null group by Year  order by Year\")\n",
    "selectall.show(n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "southeast-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|          avg(3PA)|\n",
      "+----+------------------+\n",
      "|1981|11.462809917355372|\n",
      "|1983|11.744245524296675|\n",
      "|1982|12.479892761394101|\n",
      "|1984|13.408163265306122|\n",
      "|1980|15.467787114845938|\n",
      "|1985|16.596685082872927|\n",
      "|1986| 17.61741424802111|\n",
      "|1987| 24.27777777777778|\n",
      "|1988|25.199530516431924|\n",
      "|1989|  33.0958904109589|\n",
      "|1990| 33.46623093681917|\n",
      "|1991| 37.61224489795919|\n",
      "|1992| 38.06331877729258|\n",
      "|1999| 39.96252465483235|\n",
      "|1993| 45.39866369710467|\n",
      "|1994| 47.88149688149688|\n",
      "|1998| 62.20475319926874|\n",
      "|2001| 63.85474860335196|\n",
      "|2004| 67.87692307692308|\n",
      "|2000| 68.12903225806451|\n",
      "|2012| 69.46823956442832|\n",
      "|2002|            75.034|\n",
      "|2005| 75.05299145299145|\n",
      "|2006| 76.19538188277087|\n",
      "|1996| 76.21100917431193|\n",
      "|1997| 76.67421602787456|\n",
      "|2003|  76.8136645962733|\n",
      "|1995| 79.36725663716814|\n",
      "|2008| 81.48571428571428|\n",
      "|2011|            84.664|\n",
      "|2010| 85.09342560553634|\n",
      "|2007| 85.79263565891473|\n",
      "|2009|  86.8298969072165|\n",
      "|2013| 92.42233856893543|\n",
      "|2014| 95.64320785597381|\n",
      "|2015| 97.41935483870968|\n",
      "|2016|110.53460207612457|\n",
      "|2017|122.91764705882353|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df.groupby('Year').avg('3PA').filter('avg(3PA) is not null').sort('avg(3PA)')\n",
    "df_train.show(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "liked-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "brown-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|features|          avg(3PA)|\n",
      "+--------+------------------+\n",
      "|[1981.0]|11.462809917355372|\n",
      "|[1983.0]|11.744245524296675|\n",
      "|[1982.0]|12.479892761394101|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['Year'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(res)\n",
    "v_df = v_df.select(['features', 'avg(3PA)'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "limiting-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='avg(3PA)')\n",
    "lr_model = lr.fit(v_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "solid-semester",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StructType can not accept object 2018.0 in type <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-cb07fca93646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_future = spark.createDataFrame([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m2018.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m2019.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m2020.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m2021.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1393\u001b[0m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n\u001b[0m\u001b[1;32m   1396\u001b[0m                                         % (obj, type(obj))))\n\u001b[1;32m   1397\u001b[0m         \u001b[0mverify_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: StructType can not accept object 2018.0 in type <class 'float'>"
     ]
    }
   ],
   "source": [
    "df_future = spark.createDataFrame([\n",
    "    (2018.0),\n",
    "    (2019.0),\n",
    "    (2020.0),\n",
    "    (2021.0),\n",
    "    (2022.0)\n",
    "], schema='Year float'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-lounge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
